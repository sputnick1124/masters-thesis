\chapter{Fuzzy Landing}\label{c:landing}
\section{Introduction}
With the ever-increasing proliferation of small unmanned air vehicles (sUAVs) and their use in commercial and
emergency response applications, there is a growing need for intelligent, reliable control methodologies to
safely manage their navigation, especially in possibly congested areas such as disaster areas or urban
centers. Commercial delivery companies are moving towards an automated model with reduced human operator
intervention to increase the efficiency of their deliveries. One such model consists of a vehicle-based sUAV
which departs from the delivery vehicle to make a delivery to a remote residence. Upon completing the
delivery, the sUAV returns to the vehicle and docks to receive additional packages. Considering the small
target size of a landing platform affixed to the potentially moving vehicle, and the highly dynamic conditions
in which deliveries may be accomplished, the control effort must be accurate and robust in the face of
disturbances.

Fuzzy control is able to accommodate nonlinearities in the dynamic system such as are found in the situation
of an air vehicle to ground transport rendezvous\cite{Ionita_2005}. Current approaches for
developing trajectory paths have focused on time-optimality\cite{Adams_2012}\cite{Hehn_2012} and not
necessarily on lightweight, on-board controllers. In contrast, this work optimizes for reduced control
effort, as well as computational simplicity and efficiency. Accuracy is paramount, as the target platform is
nearly equally-sized to the sUAV.

\section{System Architecture}
The research setup consists of a quadrotor aircraft of size \SI{450}{\mm} on the diagonal and a mobile rover
robot with a \SI{255}{mm} radius landing platform affixed to it (as shown in \cref{f:lezl-olli}). The
quadrotor is controlled by a Pixhawk flight controller which uses the PX4 flight control firmware. This flight
controller allows for an on-board computer to take over control of the aircraft via a serial wire connection.
A small Linux-based computer is placed on the quadrotor which sends velocity setpoints to the flight
controller. All control logic is written in Python using a collection of softwares called Robot Operating
System (ROS). ROS allows for easy integration of sensors and control actuation due to a distributed
computation framework. As a highly event-driven, publish-subscribe model, ROS maintains an accurate,
up-to-date view of internal states which are then exposed to any connected nodes.

An assumption is made that GPS (or some other global positioning) data is available for the simulation;
however, this positioning has a margin of error which is far too large to be used exclusively for precision
landing. For this reason, an on-board camera is utilized to detect and locate the target. Using the
characteristics of the camera focal length and distortion coefficients, an accurate positional error can be
obtained for the feedback control loop. 

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.6\textwidth]{images/irols.jpg}
    \caption{The test sUAV with the mobile target platform.}\label{f:lezl-olli}
\end{figure}

\subsection{Simulation}
Gazebo is a 3D simulation software which uses open source dynamics engines Bullet, Dart, and ODE to model its
components. While Gazebo has very high fidelity simulation capabilities for robots (its initial purpose), the
complexities of aerodynamics in general, and multicopter physics in particular, can only be modeled with many
simplifications. Even with the simplified dynamics, the simulation environment that Gazebo provides is very
useful for high-level controller development. Gazebo allows for the simulation of many sensor types and nicely
integrates with ROS and the PX4 flight controller firmware. The simulation
provides a real time interface for tuning the controller with visual feedback. This is the process which was
used to tune the fuzzy controller.

The most important aspect of sensing for the system is the image sensor. A camera sensor is simulated from the
underside of the quadrotor to test the efficacy and efficiency of the computer vision algorithms. Care was
taken to accurately represent the field of view and pixel noise of the physical camera sensor to be used.


\subsection{Onboard Software} ROS is an open source framework developed specifically to ease the development
of software for robotics and robotics control\cite{quigley2009ros}. Using ROS, it becomes a simple task to
distribute computational loads across a computational graph of separate nodes. Additionally, ROS has a rich
library of packages which are useful for both low-level processing of sensor information or managing hardware
interfaces, and also high-level behavioral control or localization schemes. This project use a number of such
packages to perform tasks in the areas of visual odometry\cite{olson2011tags}, kalman filtering
\cite{MooreStouchKeneralizedEkf2014}, and flight control\cite{rotors:2016,meier2015px4}. ROS allows for a
system designer to aggregate any number of processing units, called nodes, into a  complex computational graph.
An example of the graph structure created to complete this work is shown in \cref{f:rosgraph} and provides a
visualization of this complex structure. Each node in the graph represents a unit of computation which
consumes information from another node in the graph. Each edge in the graph represents a direct message
passing pipeline through which information is transmitted. Each node runs continuously and in parallel,
facilitating the creation of reactive, concurrent systems.

\begin{figure}[H]
    \centering
    \includedot[scale=0.29]{tikz/rosgraph_notf_cluster}
    \caption{Computational node graph for a typical landing simulation. Note that this graph may change over
    time as node may be dynamically started and stopped. Likewise, message passing pipelines may be opened or
    closed at any time.}\label{f:rosgraph}
\end{figure}

The nodes can be broadly categorized into three groups: Sensing nodes, Action states, and Control loop nodes.
The Sensing nodes consume image sensor output and pre-process it for Kalman filtering.
The Action states each represent a state in the state machine. These represent basic actions which the vehicle
can perform and can be aggregated to construct more complex behavior. The nodes in the Control loop represent
the plant (\verb|/mavros|), the sensor (\verb|/ekf_odom|), and the compensator (\verb|/flc_action|). In this
way, it becomes easy to visualize the canonical form of a feedback control loop. As can be seen in
\cref{f:rosgraph}, the structure allows a roboticist to build and manage highly complex systems in a
maintainable way.

\begin{figure}[H]
    \centering
    \includedot[scale=0.39]{tikz/smach}
    \caption{State machine of robotic lander}\label{f:smach}
\end{figure}

Control of the quadcopter is handled in discrete stages based on vehicle state. It is assumed that the vehicle
will have a rough estimate of the target location given to it so that it can travel to the appropriate region
and find the target in the field of view of the camera sensor. Vehicle motion from arming until target
location is handled by sending waypoints to the flight controller. Once the target is located in the image,
the vehicle gives control over to a set of FISs. The controller is described in more detail in
\cref{s:landing:controller}. The vehicle behavior over an entire mission is handled using a state
machine\cite{bohren2010smach}. Using a state machine allows the control to be handled in well-defined domains
and ensures that transitions between states are handled smoothly. The states comprising a full mission from
takeoff to landing are shown in \cref{f:smach}. A link to the video showing a full landing mission can be
found in the references section\cite{yt_stat}. An annotated image of the simulation at each state can be found
in \cref{app:smach} (example in \cref{f:sim_static_shot}).

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{images/static_captures/static-15h39m54s115}
    \caption{Image of the simulation and live state diagram while vehicle is approaching the
    platform.}\label{f:sim_static_shot}
\end{figure}

The state of the vehicle is managed by fusing together the positional estimate from the camera sensor and the
AprilTag estimate (see \cref{s:landing:cv}), as well as orientation information from the onboard IMU using an
extended kalman filter (EKF). This estimate is only valid when the vehicle has a visual track on the target
platform; otherwise, it is assumed that the vehicle is still in transit from its launch location, or it has
landed. 

As can be seen in \cref{f:smach}, a mission starts in the ``ARM'' by arming the vehicle and immediately
sending it a waypoint which is near the target's location. Once the vehicle has received the waypoint, it
enters the ``TRACK'' state and is en route to the target location. While in this state, it monitors the
quality of its visual estimation of the target location by evaluating the norm of the covariance matrix 
computed by the EKF. Only when the covariance is sufficiently small does the vehicle transition to the next
state.

The transition to the ``APPROACH\_LAND'' state signals the transfer of control from the flight controller's
waypoint manager to the FISs. The "APPROACH\_LAND" state is simply a composition of two substates, "APPROACH"
and "LAND". During the "APPROACH" substate, the EKF is still running, sending estimates to the fuzzy logic
controller. The fuzzy logic controller is sending velocity setpoints to the onboard flight controller, thus
closing the feedback loop. The desired outcome of this state is to get the vehicle in a position above
the platform such that we can initiate the landing sequence. When the vehicle meets a proximity threshold,
it transitions to the ``LAND'' substate and puts down onto the landing pad. The details of the simulation,
control, vision estimation, and development process are discussed at length in the following sections.

\subsection{Computer Vision}\label{s:landing:cv}
Image processing is handled by a node in the ROS computation graph that is constantly processing image outputs
from the onboard camera. Once it senses the platform, it then publishes
a state estimation to the rest of the nodes in the ROS graph. Much emphasis is put on the sensing algorithms to be computationally efficient to decrease the load on the
on-board computer. For this purpose, only a small number of image processes are required to detect and locate
the target. As a first pass, the image is brought into the Hue-Saturation-Value (HSV) color space. This has
been shown to be a robust space in which to perform color detection and segmentation in uncontrolled and
unpredictable lighting conditions\cite{zhao2002robust}. A simple thresholding is performed on the image to
isolate a sufficiently wide band of yellows to match the color of the target and dilate this to a binary blob.
From this binary image, the image moments are calculated by
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/rs_working_apriltags_crop.png}
    \caption{Simulated image sensor detection of AprilTag marker.}\label{f:apriltag}
\end{figure}

\begin{equation}\label{e:im_moments} 
    m_{ij}=\sum_{x,y}x^iy^jI_{xy}
\end{equation}
\nomenclature[]{$m_{ij}$}{Raw image moment}
\nomenclature[]{$I_{xy}$}{Pixel intensity value}
where $I_{xy}$ is the pixel intensity value for each pixel $(x,y)$ (equal to 1 for this binary blob) and $i,j
= 0,1,2$. From this it can be seen that $m_{00}$ describes the area and $\frac{m_{10}}{m_{00}}$ and
$\frac{m_{01}}{m_{00}}$ describe the centroids $\overline{x}_p$ and $\overline{y}_p$ in terms of
pixels\cite{hu1962visual}. The blob is assumed to be circular and hence a diameter is extracted from the pixel
area. Using the focal length of the sensor, these image points are then projected onto the ground plane using
the known diameter of the landing pad and a vertical offset estimate is obtained as is shown in
\cref{e:dist_est}.
\begin{equation}\label{e:dist_est}
    d_z=\frac{d\cdot f}{m\cdot d_p}
\end{equation}
where $f$ is the focal length of the camera in units of length, $d$ is the known diameter of the landing pad,
$d_p$ is the estimated diameter in pixels, and $m$ represents a scaling factor in units of \si{\px\per\mm}
(unity in the simulation).
\nomenclature[]{$f$}{Focal length of image sensor}
\nomenclature[]{$m$}{Pixel scaling factor}
Assuming that the image plane and the ground plane are parallel, the center of the image can be assumed to
point directly below the vehicle and the horizontal offsets to the landing pad can then be calculated as
\begin{align}\label{e:horiz_est}
    d_x &= d_z\cdot\frac{m\overline{x}_p}{f}\\
    d_y &= d_z\cdot\frac{m\overline{y}_p}{f}\label{e:horiz_est-y}
\end{align}
\nomenclature[]{$d_x$}{Horizontal offset error from vehicle to target in the body-fixed $x$-axis}
\nomenclature[]{$d_y$}{Horizontal offset error from vehicle to target in the body-fixed $y$-axis}
\nomenclature[]{$d_z$}{Vertical offset error from vehicle to the target}

Unless the camera is mounted to a perfect gimbal, the image plane cannot be assumed to be parallel to the
ground plane, thus invalidating \crefrange{e:horiz_est}{e:horiz_est-y}. To overcome this, the rotation
sequence needed to transform the image into body-relative coordinates is found and applied to the image data.
The camera is assumed to be rigidly affixed to the body of the vehicle, thus this is a simple static
transformation. Then, the vehicle body frame is related to the inertial frame by another rotation. The landing
pad is assumed to be on level ground and the distance between the vehicle and pad is estimated from
\cref{e:dist_est}. Note that although there will be image skew when the vehicle is under some rotation, the error in estimated distance this introduces is small compared to the actual distance of the vehicle from
the target. As the vehicle approaches the target, the estimate improves in quality due to a crisper image,
level flight, and the addition of AprilTag estimation.

In order to account for vehicle attitude, a rotation sequence is applied to the projected target location.
Let $R\left(\phi, \theta, \psi\right)$ be the rotation matrix defined as
\begin{equation}
    R(\phi,\theta,\psi) = 
    \begin{bmatrix}
            c_{\theta}c_{\psi}                           & -c_{\theta}s_{\phi}                           & s_{\theta}\\
            c_{\phi}s_{\psi} + c_{\psi}s_{\phi}s_{\theta} & c_{\phi}c_{\psi} - c_{\psi}s_{\phi}s_{\theta} & -c_{\theta}s_{\phi}\\
            s_{\phi}s_{\psi} -c_{\phi}c_{\psi}s_{\theta} & c_{\psi}s_{\phi} + c_{\phi}s_{\theta}s_{\psi} & c_{\phi}c_{\theta}
    \end{bmatrix}
\end{equation}
where $\phi$, $\theta$, and $\psi$ are vehicle roll, pitch and yaw respectively. $c_{\alpha}$ and $s_{\alpha}$
represent $\cos{\alpha}$ and $\sin{\alpha}$ respectively. Let $R_x^y$ represent the rotation from frame $x$ to
$y$, then $R_x^yR_y^z$ is the rotation from frame $x$ to frame $z$ with respect to the stationary frame $x$.
Now assuming the vehicle can estimate its orientation from the onboard inertial measurement unit, the
following rotation matrices can be defined.
\begin{align}
    R_{cam}^{body}   & = R(\pi, 0, 0)\label{e:rot_cam_bod}\\
    R_{body}^{inert} & = R(\phi, \theta, \psi)\label{e:rot_body_inert}
\end{align}
where $R_{x}^{y}$ represents the rotation from frame $x$ to frame $y$. These rotations are then prefixed to the
projected point found  in \crefrange{e:dist_est}{e:horiz_est-y} to find the point in world units with respect
to the vehicle.
\begin{align}\label{e:rotate}
    P_i = \begin{pmatrix}d_x\\d_y\\d_z\end{pmatrix}\\
    P_r = R_{body}^{inert}R_{cam}^{body}P_i
\end{align}
where $P_i$ is the projected point of the target in the plane parallel to the image plane, and $P_r$ is the
rotated point of the target with respect to the vehicle.

As the vehicle approaches the landing pad, the image field of view is overtaken by the landing pad itself and
the former segmentation no longer becomes effective. For this purpose, an AprilTag\cite{olson2011tags} (see
\cref{f:apriltag}) is placed on the center of the target. In \cref{f:landing_ims}, a landing approach is
illustrated from the viewpoint of the image sensor. This figure shows three different stages of the image
processing pipeline: raw image capture ( \crefrange{f:colora}{f:colorb}), AprilTag detection
(\crefrange{f:aprila}{f:aprilb}), and platform detection (\crefrange{f:cva}{f:cvb}). Individual estimates from
both the AprilTag detection and platform detection are then fused using an EKF (\cref{s:ekf}). 

\begin{figure}
    \begin{subfigmatrix}{4}% number of columns
        \subfigure[\label{f:colora}]{\includegraphics[width=0.2\textwidth]{images/image1_18469000.png}}
        \subfigure[]{\includegraphics[width=0.2\textwidth]{images/image1_30863000.png}}
        \subfigure[]{\includegraphics[width=0.2\textwidth]{images/image1_36074000.png}}
        \subfigure[\label{f:colorb}]{\includegraphics[width=0.2\textwidth]{images/image1_38233000.png}}
        \subfigure[\label{f:aprila}]{\includegraphics[width=0.2\textwidth]{images/image2_18469000.png}}
        \subfigure[]{\includegraphics[width=0.2\textwidth]{images/image2_30863000.png}}
        \subfigure[]{\includegraphics[width=0.2\textwidth]{images/image2_36074000.png}}
        \subfigure[\label{f:aprilb}]{\includegraphics[width=0.2\textwidth]{images/image2_38233000.png}}
        \subfigure[\label{f:cva}]{\includegraphics[width=0.2\textwidth]{images/image1_18469000_proc.png}}
        \subfigure[]{\includegraphics[width=0.2\textwidth]{images/image1_30863000_proc.png}}
        \subfigure[]{\includegraphics[width=0.2\textwidth]{images/image1_36074000_proc.png}}
        \subfigure[\label{f:cvb}]{\includegraphics[width=0.2\textwidth]{images/image1_38233000_proc.png}}
    \end{subfigmatrix}
    \caption{A time series of images taken during the landing maneuver.}
    \label{f:landing_ims}
\end{figure}

\section{Controller}\label{s:landing:controller}
Control is actuated on the vehicle by providing velocity setpoints to the flight controller, $v_x$, $v_y$,
$v_z$, and yaw rate, $\dot{\psi}$. Ideally either linear acceleration setpoints or forces would be the control
output, but limitations in the flight controller make this impossible at this time. Setting velocities allows
the controller to cope with step changes to position offset well, but have difficulties tracking ramps. The
yaw angle, $\psi$ of the vehicle at landing is irrelevant for this project, but the controller is able to
control that axis regardless.

\subsection{PID Controller}
A PID controller was created which uses error estimates and sends velocity setpoints to the flight controller.
Due to the lack of a mathematical model of the system, the PID gains were found by iterating the simulation
with various position setpoints and dynamically adjusting the gains while observing the response visually.
This closely mimics the method by which PID gains are attained in the tuning of an actual quadrotor by flying
a series of test flights and adjusting gain values by feel. In this way, a reasonable response and landing
sequence was achieved for first a static target and then repeated for a dynamic target moving with constant
velocity. The results are presented in \crefrange{f:pid_stat}{f:pid_dyn} in which the normed horizontal offset
from target is shown in conjunction with the vertical distance to the platform. This view of the data gives an
overall sens of how the controller performs by showing how smooth the trajectory of the vehicle is as it
approaches the platform.  Viewing the charts, it is
apparent that as the vehicle approaches the target, it tends to accumulate horizontal errors and must correct
more frequently, particularly in the dynamic case. In both cases, the quadrotor managed to successfully land
on the target. In the static case, the final offset from the center of the target was less than \SI{5}{\cm}.
For the dynamic case, the error was \SI{7}{\cm} which nearly displaced it from the target surface. In both
cases, the sink rate and yaw rates were controlled by simple PD controllers. It can be seen that the controller took a significantly longer time to intercept and land
on the moving target as it repeatedly had trouble responding to the motion and would temporarily lose its
visual track. This is apparent in the large spiking errors in \cref{f:pid_dyn}.

%\begin{figure}
    %\centering
    %\input{static_smach_good_plot}
    %\caption{Fuzzy-controlled static target interception.}\label{f:fuz_stat}
%\end{figure}

%\begin{figure}
    %\centering
    %\input{moving_line_good_plot}
    %\caption{Fuzzy-controlled dynamic target interception.}\label{f:fuz_dyn}
%\end{figure}

\begin{figure}
    \centering
    \input{pid_static}
    %\includegraphics[width=0.9\textwidth]{images/mag_z_pid_static.png}
    \caption{$(kp,ki,kd)=(2.1,0.015,0.2)$ for static target interception.}\label{f:pid_stat}
\end{figure}

\begin{figure}
    \centering
    \input{pid_dyn}
    %\includegraphics[width=0.9\textwidth]{images/mag_z_pid_dynamic.png}
    \caption{$(k_p,k_i,k_d)=(1.8,0.012,0.02)$ for dynamic target interception. The target is moving with a
    velocity of \SI{0.1}{\meter\per\s} in a straight line.}\label{f:pid_dyn}
\end{figure}


\subsection{Kalman Filter}\label{s:ekf}
In order to reduce the deleterious effect of instantaneous error accumulation when losing a visual track, a
kalman filter is introduced.  The filter provides a continuous, fused estimate of the vehicle's position
relative to the platform whenever there is at least visual sensing of the platform. This is done using the
\verb|robot_localization| package from ROS\cite{MooreStouchKeneralizedEkf2014}. This package is an
implementation of an extended kalman filter (EKF). The EKF provides a means by which to predict the vehicle
state even in the absence of current measurement\cite{kalman1960new}. The prediction is then corrected when
new measurement values are obtained. The measurements used to update the filter state are 1) the estimate from
the computer vision algorithm to update the vehicle position, 2) the estimate from the
AprilTag\cite{olson2011tags} routine to update both position and orientation, and 3) the IMU to
update angular rates and linear accelerations. These measurements are fused together to build an internal
model estimate of the dynamics of the vehicle. It is this model which provides the predictions between
measurements. An example of the EKF measurement is shown in \cref{f:ekf_plot}. It can be seen in this data
that though the visual estimate is very noisy, the EKF fusion follows the truth signal closely even between
the infrequent update periods of the accurate AprilTag measurement.

\begin{figure}[ht]
    \centering
    \input{tikz/ekf_plot}
    \caption{EKF estimate with input estimates over short time sample.}\label{f:ekf_plot}
\end{figure}

\Cref{app:moving} contains annotated screenshots (example in \cref{f:sim_moving_shot}) of a video of a
simulated vehicle landing on a moving platform. This video also includes a visualization of the EKF state over
time and various estimates which are fused together. A link to this video is found in the references
section\cite{yt_dyn}.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{images/moving_capture/moving-18h12m15s888.png}
    \caption{Image of the simulation environment and along with estimate visualization and camera
    feeds}\label{f:sim_moving_shot}
\end{figure}


\subsection{Fuzzy Controller}
Four fuzzy controllers of similar architecture were created as shown in \cref{f:fuzzy_mfs}.
Each input fuzzy partition was multiplied by a scaling factor to bring it into the range of the controller.
Likewise, the outputs were then again scaled to match actuation limits. The rule base was developed using
common intuition about the system dynamics. A full rule matrix was defined to fully cover the system
possibilities. This rule matrix is shown in \cref{t:rules}.
\begin{figure}[ht]
    \begin{subfigmatrix}{2}
        \subfigure[Error input \label{f:error_in_mfs}]  {\resizebox{0.48\textwidth}{!}{\input{tikz/land_in1}}}
        \subfigure[Error rate input \label{f:error_d_in_mfs}]{\resizebox{0.48\textwidth}{!}{\input{tikz/land_in2}}}
        \subfigure[Rate command output \label{f:vel_out_mfs}]   {\resizebox{0.48\textwidth}{!}{\input{tikz/land_out}}}
    \end{subfigmatrix}
    \caption{Membership function definitions for fuzzy logic controller.}\label{f:fuzzy_mfs}
\end{figure}

These rules provide a process by which to lend the controller a
decision-making system with a foundation in human reasoning. The tuning process of the fuzzy controller then
becomes the task of defining the membership functions which decide how much of each rule should be activated
for certain inputs. Triangular membership functions are used exclusively for their simplicity in definition
and tuning\cite{mishra1994performance} while the aggregation of rules is the popular min-max method put forth
by Mamdani\cite{MAMDANI19751}. The membership functions shown in \cref{f:fuzzy_mfs} are the result of a number
of iterative tuning steps and were found to be the most effective of the configurations attempted.


\begin{table}[ht]
    \centering
    \caption{Fuzzy rule base. The error and error rate membership sets correspond to velocity output
    membership sets according to this table. N-negative, SN-small negative, Z-zero, SP-small positive,
P-positive}re intuitive and easy to understand and pr\label{t:rules}
    \begin{tabular}{cc||c|c|c|c|c|}
        &  \multicolumn{6}{c}{error}  \\ 	
        \multirow{6}{*}{error rate} &    & N  & SN & Z  & SP & P  \\ 	\hhline{~=#=|=|=|=|=|}
                                    & N  & P  & P  & SP & SP & Z  \\ 	\cline{2-7}
                                    & SN & P  & SP & SP & Z  & SN \\ 	\cline{2-7}
                                    & Z  & SP & SP & Z  & SN & SN \\ 	\cline{2-7}
                                    & SP & SP & Z  & SN & SN & N  \\ 	\cline{2-7}
                                    & P  & Z  & SN & SN & N  & N  \\ 	\cline{2-7}
    \end{tabular}
\end{table}



The result of this controller was sufficient to land the quadrotor on both the static and
constant linear velocity dynamic platforms. \Crefrange{f:fuz_stat}{f:fuz_dyn} show the results of the static
and dynamic landings using the fuzzy controllers. These figures begin at takeoff of the vehicle, and it is
clearly seen where the controller locks onto the visual image and initiates the landing state. It is also
clearly seen that with the implementation of both the kalman filter and the fuzzy controller, the errors
resulting from image loss are eliminated, providing a smoother landing approach.

\begin{figure}
    \centering
    \input{static_smach_good_plot}
    \caption{Fuzzy-controlled static target interception.}\label{f:fuz_stat}
\end{figure}

\begin{figure}
    \centering
    \input{moving_line_good_plot}
    \caption{Fuzzy-controlled dynamic target interception. The target is moving in a straight line with a
    velocity of \SI{0.1}{\meter\per\second}.}\label{f:fuz_dyn}
\end{figure}


%\begin{figure}[ht]
    %\centering
    %\begin{subfigmatrix}{2}
        %\subfigure[Static target interception.\label{f:fuz_stat}]{\includegraphics[width=0.49\textwidth]{static_smach_good_plot}}
        %\subfigure[Dynamic target interception.\label{f:fuz_dyn}]{\includegraphics[width=0.49\textwidth]{moving_line_good_plot}}
    %\end{subfigmatrix}
    %\caption{Fuzzy controllers for static and dynamic landing}\label{f:fuzzy_lands}
%\end{figure}


\section{Conclusion}
It has been shown that a pure FLS controller is capable of controlling the position of a multirotor vehicle in
the task of precision landing on a small target. The small size of the platform presents a challenging
landing target; the FLS proved capable enough to overcome this challenge easily. The kalman filtering was an
integral component in providing the controller with a steady estimate of its error state. The components
chosen to accomplish this task are readily available in university laboratory environments and the physical
realization of this system was kept in the forefront of the design process. 

